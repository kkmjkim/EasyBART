# EasyBART (Extractive/Abstractive summarY BART)

Transformer-based language models have shown tremendous performances on various natural language processing tasks including text classification, machine translation (MT), question answering (QA), and even natural language generation (NLG). These methods rely entirely on training an end-to-end deep learning model. The model is initially trained on a large corpus of a target language using masked language modeling (MLM), which aims at learning representations and semantic structures of the langauge (pre-training stage). Weights of the trained model are be used as a warm starting point, which can be fine-tuned with a relatively small dataset for various downstream tasks (fine-tuning stage).

Among many promising language models, BART successfully combines BERT's encoder architecture with GPT's autoregressive decoder architecture. This allows BART to focus more on language generation rather than classification and perform well on language generation task compared to the previous methods. 
